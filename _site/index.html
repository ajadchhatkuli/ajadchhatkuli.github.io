<!DOCTYPE HTML>
<html lang="en">

<head>
  <title>Ajad Chhatkuli</title>

  <meta content="text/html; charset=utf-8" http-equiv="Content-Type">

  <meta name="author" content="Ajad Chhatkuli" />
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="/style.css" />
  <link rel="canonical" href="http://localhost:4000/">
  <link href="https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css">

</head>



<body>
  <div class="site">
    <div class="site__inner">
      <header class="hero" id="about">
        <div class="hero__intro">
          <h1 class="hero__title">
            Ajad Chhatkuli
          </h1>
          <p>I am a research scientist at <a href="https://insait.ai/">INSAIT</a>, previously a senior scientist and postdoc at the Computer Vision Lab in ETH Zurich, in the group of <a href="https://ee.ethz.ch/the-department/faculty/professors/person-detail.OTAyMzM=.TGlzdC80MTEsMTA1ODA0MjU5.html">Prof. Luc Van Gool</a>. I completed my PhD <a href="https://tel.archives-ouvertes.fr/tel-01491344">thesis</a> under the supervision of <a href="http://igt.ip.uca.fr/~ab/">Prof. Adrien Bartoli</a> and <a href="https://www.uah.es/en/estudios/profesor/Daniel-Pizarro-Perez/">Prof. Daniel Pizarro</a> in the field of non-rigid 3D reconstruction. Currently my research scope is diverse, though I have two main focus areas: i) 3D/4D scene understanding and generation  ii) video understanding.</p>
          <nav class="hero__links" aria-label="Contact links">
            <a class="hero__link" href="mailto:ajad.chhatkuli@vision.ee.ethz.ch">Email</a>
            <a class="hero__link" href="https://github.com/ajadchhatkuli">GitHub</a>
            <a class="hero__link" href="https://scholar.google.com/citations?user=3BHMHU4AAAAJ&hl=en">Google Scholar</a>
          </nav>
        </div>
        <div class="hero__photo">
          <img alt="profile photo of Ajad Chhatkuli" src="images/mypic.JPG">
        </div>
      </header>

      <main class="content">
        <section class="section section--publications" id="publications">
          <h2 class="section__title">Publications</h2>
          <div class="pub-list">
            
            
            <article class="pub-card">
              
              <div class="pub-card__media">
                <img src="/tn/images/egonight-teaser-720.png" alt="EgoNight: Towards Egocentric Vision Understanding at Night with a Challenging Benchmark thumbnail">
              </div>
              
              <div class="pub-card__body">
                <h3 class="pub-card__title">EgoNight: Towards Egocentric Vision Understanding at Night with a Challenging Benchmark</h3>
                <p class="pub-card__authors">Deheng Zhang, Yuqian Fu, Runyi Yang, Yang Miao, Tianwen Qian, Xu Zheng, Guolei Sun, <strong>Ajad Chhatkuli</strong>, Xuanjing Huang, Yugen Jiang, L. V. Gool, D. Paudel</p>
                <p class="pub-card__meta">
                  <em>arXiv</em>, 2025
                  
                </p>
                
                <div class="pub-card__actions">
                  <div class="pub-card__links">
                    
                    
                    
                    <a class="pub-link pub-link--text" href="https://arxiv.org/abs/2510.06218">arxiv/pdf</a>
                    
                    
                    <details class="pub-card__details">
                      <summary class="pub-card__toggle">
                        <span>More</span>
                        <span class="pub-card__toggle-icon" aria-hidden="true">⏬</span>
                      </summary>
                      <div class="pub-card__excerpt">
                        <p>Most existing benchmarks for egocentric vision understanding focus primarily on daytime scenarios,
overlooking the low-light conditions that are inevitable in real-world applications.</p>


                      </div>
                    </details>
                    
                  </div>
                  
                </div>
                
              </div>
            </article>
            
            
            
            <article class="pub-card">
              
              <div class="pub-card__media">
                <img src="/tn/images/enhancing-semantic-segmentation-continual.png" alt="Enhancing Semantic Segmentation with Continual Self-Supervised Pre-training thumbnail">
              </div>
              
              <div class="pub-card__body">
                <h3 class="pub-card__title">Enhancing Semantic Segmentation with Continual Self-Supervised Pre-training</h3>
                <p class="pub-card__authors">Brown Ebouky, <strong>Ajad Chhatkuli</strong>, C. Malossi, Christoph Studer, Roy Assaf, A. Bartezzaghi</p>
                <p class="pub-card__meta">
                  <em>arXiv</em>, 2025
                  
                </p>
                
                <div class="pub-card__actions">
                  <div class="pub-card__links">
                    
                    
                    
                    <a class="pub-link pub-link--text" href="https://arxiv.org/abs/2509.17816">arxiv/pdf</a>
                    
                    
                    <details class="pub-card__details">
                      <summary class="pub-card__toggle">
                        <span>More</span>
                        <span class="pub-card__toggle-icon" aria-hidden="true">⏬</span>
                      </summary>
                      <div class="pub-card__excerpt">
                        <p>Self-supervised learning (SSL) has emerged as a central paradigm for training foundation models by
leveraging large-scale unlabeled datasets, often producing representations with strong
generalization capabilities.</p>


                      </div>
                    </details>
                    
                  </div>
                  
                </div>
                
              </div>
            </article>
            
            
            
            <article class="pub-card">
              
              <div class="pub-card__media">
                <img src="/tn/images/one2any-one-reference-6d-pose-estimation-for-any-object.png" alt="One2Any: One-Reference 6D Pose Estimation for Any Object thumbnail">
              </div>
              
              <div class="pub-card__body">
                <h3 class="pub-card__title">One2Any: One-Reference 6D Pose Estimation for Any Object</h3>
                <p class="pub-card__authors">Mengya Liu, Siyuan Li, <strong>Ajad Chhatkuli</strong>, Prune Truong, L. V. Gool, F. Tombari</p>
                <p class="pub-card__meta">
                  <em>Computer Vision and Pattern Recognition</em>, 2025
                  
                </p>
                
                <div class="pub-card__actions">
                  <div class="pub-card__links">
                    
                    
                    
                    <a class="pub-link pub-link--text" href="https://arxiv.org/abs/2505.04109">arxiv/pdf</a>
                    
                    
                    <details class="pub-card__details">
                      <summary class="pub-card__toggle">
                        <span>More</span>
                        <span class="pub-card__toggle-icon" aria-hidden="true">⏬</span>
                      </summary>
                      <div class="pub-card__excerpt">
                        <p>6D object pose estimation remains challenging for many applications due to dependencies on complete
3D models, multi-view images, or training limited to specific object categories.</p>


                      </div>
                    </details>
                    
                  </div>
                  
                </div>
                
              </div>
            </article>
            
            
            
            <article class="pub-card">
              
              <div class="pub-card__media">
                <img src="/tn/images/self-supervised-shape-completion-invo.png" alt="Self-supervised Shape Completion via Involution and Implicit Correspondences thumbnail">
              </div>
              
              <div class="pub-card__body">
                <h3 class="pub-card__title">Self-supervised Shape Completion via Involution and Implicit Correspondences</h3>
                <p class="pub-card__authors">Mengya Liu, <strong>Ajad Chhatkuli</strong>, Janis Postels, L. V. Gool, F. Tombari</p>
                <p class="pub-card__meta">
                  <em>European Conference on Computer Vision</em>, 2024
                  
                </p>
                
                <div class="pub-card__actions">
                  <div class="pub-card__links">
                    
                    
                    
                    <a class="pub-link pub-link--text" href="https://arxiv.org/abs/2409.15939">arxiv/pdf</a>
                    
                    
                    <details class="pub-card__details">
                      <summary class="pub-card__toggle">
                        <span>More</span>
                        <span class="pub-card__toggle-icon" aria-hidden="true">⏬</span>
                      </summary>
                      <div class="pub-card__excerpt">
                        <p>3D shape completion is traditionally solved using supervised training or by distribution learning on
complete shape examples.</p>


                      </div>
                    </details>
                    
                  </div>
                  
                </div>
                
              </div>
            </article>
            
            
            
            <article class="pub-card">
              
              <div class="pub-card__media">
                <img src="/tn/images/vf-nerf-learning-neural-vector-fields-for-indoor-scene-reconstruction.png" alt="VF-NeRF: Learning Neural Vector Fields for Indoor Scene Reconstruction thumbnail">
              </div>
              
              <div class="pub-card__body">
                <h3 class="pub-card__title">VF-NeRF: Learning Neural Vector Fields for Indoor Scene Reconstruction</h3>
                <p class="pub-card__authors">Albert Gassol Puigjaner, Edoardo Mello Rella, Erik Sandström, <strong>Ajad Chhatkuli</strong>, L. V. Gool</p>
                <p class="pub-card__meta">
                  <em>arXiv.org</em>, 2024
                  
                </p>
                
                <div class="pub-card__actions">
                  <div class="pub-card__links">
                    
                    
                    
                    <a class="pub-link pub-link--text" href="https://arxiv.org/abs/2408.08766">arxiv/pdf</a>
                    
                    
                    <details class="pub-card__details">
                      <summary class="pub-card__toggle">
                        <span>More</span>
                        <span class="pub-card__toggle-icon" aria-hidden="true">⏬</span>
                      </summary>
                      <div class="pub-card__excerpt">
                        <p>Implicit surfaces via neural radiance fields (NeRF) have shown surprising accuracy in surface
reconstruction.</p>


                      </div>
                    </details>
                    
                  </div>
                  
                </div>
                
              </div>
            </article>
            
            
            
            <article class="pub-card">
              
              <div class="pub-card__media">
                <img src="/tn/images/ihuman-instant-animatable-digital-humans-from-monocular-videos.png" alt="iHuman: Instant Animatable Digital Humans From Monocular Videos thumbnail">
              </div>
              
              <div class="pub-card__body">
                <h3 class="pub-card__title">iHuman: Instant Animatable Digital Humans From Monocular Videos</h3>
                <p class="pub-card__authors">Pramish Paudel, Anubhav Khanal, <strong>Ajad Chhatkuli</strong>, D. Paudel, Jyoti Tandukar</p>
                <p class="pub-card__meta">
                  <em>European Conference on Computer Vision</em>, 2024
                  
                </p>
                
                <div class="pub-card__actions">
                  <div class="pub-card__links">
                    
                    
                    
                    <a class="pub-link pub-link--text" href="https://arxiv.org/abs/2407.11174">arxiv/pdf</a>
                    
                    
                    <details class="pub-card__details">
                      <summary class="pub-card__toggle">
                        <span>More</span>
                        <span class="pub-card__toggle-icon" aria-hidden="true">⏬</span>
                      </summary>
                      <div class="pub-card__excerpt">
                        <p>Personalized 3D avatars require an animatable representation of digital humans.</p>


                      </div>
                    </details>
                    
                  </div>
                  
                </div>
                
              </div>
            </article>
            
            
            
            <article class="pub-card">
              
              <div class="pub-card__media">
                <img src="/tn/images/optimizing-long-term-robot-tracking-with-multi-platform-sensor-fusion.png" alt="Optimizing Long-Term Robot Tracking with Multi-Platform Sensor Fusion thumbnail">
              </div>
              
              <div class="pub-card__body">
                <h3 class="pub-card__title">Optimizing Long-Term Robot Tracking with Multi-Platform Sensor Fusion</h3>
                <p class="pub-card__authors">Giuliano Albanese, Arka Mitra, Jan-Nico Zaech, Yupeng Zhao, <strong>Ajad Chhatkuli</strong>, L. V. Gool</p>
                <p class="pub-card__meta">
                  <em>IEEE Workshop/Winter Conference on Applications of Computer Vision</em>, 2024
                  
                </p>
                
                <div class="pub-card__actions">
                  <div class="pub-card__links">
                    
                    
                    
                    <a class="pub-link pub-link--text" href="https://openaccess.thecvf.com/content/WACV2024/papers/Albanese_Optimizing_Long-Term_Robot_Tracking_With_Multi-Platform_Sensor_Fusion_WACV_2024_paper.pdf">pdf</a>
                    
                    
                    <details class="pub-card__details">
                      <summary class="pub-card__toggle">
                        <span>More</span>
                        <span class="pub-card__toggle-icon" aria-hidden="true">⏬</span>
                      </summary>
                      <div class="pub-card__excerpt">
                        <p>Monitoring a fleet of robots requires stable long-term tracking with re-identification, which is yet
an unsolved challenge in many scenarios.</p>


                      </div>
                    </details>
                    
                  </div>
                  
                </div>
                
              </div>
            </article>
            
            
            
            <article class="pub-card">
              
              <div class="pub-card__media">
                <img src="/tn/images/ms-evs-multispectral-event-based-vision-for-deep-learning-based-face-detection.png" alt="MS-EVS: Multispectral event-based vision for deep learning based face detection thumbnail">
              </div>
              
              <div class="pub-card__body">
                <h3 class="pub-card__title">MS-EVS: Multispectral event-based vision for deep learning based face detection</h3>
                <p class="pub-card__authors">Saad Himmi, Vincent Parret, <strong>Ajad Chhatkuli</strong>, L. V. Gool</p>
                <p class="pub-card__meta">
                  <em>IEEE Workshop/Winter Conference on Applications of Computer Vision</em>, 2024
                  
                </p>
                
                <div class="pub-card__actions">
                  <div class="pub-card__links">
                    
                    
                    
                    <a class="pub-link pub-link--text" href="https://openaccess.thecvf.com/content/WACV2024/papers/Himmi_MS-EVS_Multispectral_Event-Based_Vision_for_Deep_Learning_Based_Face_Detection_WACV_2024_paper.pdf">pdf</a>
                    
                    
                    <details class="pub-card__details">
                      <summary class="pub-card__toggle">
                        <span>More</span>
                        <span class="pub-card__toggle-icon" aria-hidden="true">⏬</span>
                      </summary>
                      <div class="pub-card__excerpt">
                        <p>Event-based sensing is a relatively new imaging modality that enables low latency, low power, high
temporal resolution and high dynamic range acquisition.</p>


                      </div>
                    </details>
                    
                  </div>
                  
                </div>
                
              </div>
            </article>
            
            
            
            <article class="pub-card">
              
              <div class="pub-card__media">
                <img src="/tn/images/residual-learning-for-image-point-descriptors.png" alt="Residual Learning for Image Point Descriptors thumbnail">
              </div>
              
              <div class="pub-card__body">
                <h3 class="pub-card__title">Residual Learning for Image Point Descriptors</h3>
                <p class="pub-card__authors">Rashik Shrestha, <strong>Ajad Chhatkuli</strong>, Menelaos Kanakis, L. V. Gool</p>
                <p class="pub-card__meta">
                  <em>arXiv.org</em>, 2023
                  
                </p>
                
                <div class="pub-card__actions">
                  <div class="pub-card__links">
                    
                    
                    
                    <a class="pub-link pub-link--text" href="https://arxiv.org/abs/2312.15471">arxiv/pdf</a>
                    
                    
                    <details class="pub-card__details">
                      <summary class="pub-card__toggle">
                        <span>More</span>
                        <span class="pub-card__toggle-icon" aria-hidden="true">⏬</span>
                      </summary>
                      <div class="pub-card__excerpt">
                        <p>Local image feature descriptors have had a tremendous impact on the development and application of
computer vision methods.</p>


                      </div>
                    </details>
                    
                  </div>
                  
                </div>
                
              </div>
            </article>
            
            
            
            <article class="pub-card">
              
              <div class="pub-card__media">
                <img src="/tn/images/continuous-pose-for-monocular-cameras-in-neural-implicit-representation.png" alt="Continuous Pose for Monocular Cameras in Neural Implicit Representation thumbnail">
              </div>
              
              <div class="pub-card__body">
                <h3 class="pub-card__title">Continuous Pose for Monocular Cameras in Neural Implicit Representation</h3>
                <p class="pub-card__authors">Qi Ma, D. Paudel, <strong>Ajad Chhatkuli</strong>, L. V. Gool</p>
                <p class="pub-card__meta">
                  <em>Computer Vision and Pattern Recognition</em>, 2023
                  
                </p>
                
                <div class="pub-card__actions">
                  <div class="pub-card__links">
                    
                    
                    
                    <a class="pub-link pub-link--text" href="https://arxiv.org/abs/2311.17119">arxiv/pdf</a>
                    
                    
                    <details class="pub-card__details">
                      <summary class="pub-card__toggle">
                        <span>More</span>
                        <span class="pub-card__toggle-icon" aria-hidden="true">⏬</span>
                      </summary>
                      <div class="pub-card__excerpt">
                        <p>In this paper, we showcase the effectiveness of optimizing monocular camera poses as a continuous
function of time.</p>


                      </div>
                    </details>
                    
                  </div>
                  
                </div>
                
              </div>
            </article>
            
            
            
            <article class="pub-card">
              
              <div class="pub-card__media">
                <img src="/tn/images/deformable-neural-radiance-fields-using-rgb-and-event-cameras.png" alt="Deformable Neural Radiance Fields using RGB and Event Cameras thumbnail">
              </div>
              
              <div class="pub-card__body">
                <h3 class="pub-card__title">Deformable Neural Radiance Fields using RGB and Event Cameras</h3>
                <p class="pub-card__authors">Qi Ma, D. Paudel, <strong>Ajad Chhatkuli</strong>, L. Gool</p>
                <p class="pub-card__meta">
                  <em>IEEE International Conference on Computer Vision</em>, 2023
                  
                </p>
                
                <div class="pub-card__actions">
                  <div class="pub-card__links">
                    
                    
                    
                    <a class="pub-link pub-link--text" href="https://arxiv.org/abs/2309.08416">arxiv/pdf</a>
                    
                    
                    <details class="pub-card__details">
                      <summary class="pub-card__toggle">
                        <span>More</span>
                        <span class="pub-card__toggle-icon" aria-hidden="true">⏬</span>
                      </summary>
                      <div class="pub-card__excerpt">
                        <p>Modeling Neural Radiance Fields for fast-moving deformable objects from visual data alone is a
challenging problem.</p>


                      </div>
                    </details>
                    
                  </div>
                  
                </div>
                
              </div>
            </article>
            
            
            
            <article class="pub-card">
              
              <div class="pub-card__media">
                <img src="/tn/images/unsupervised-template-warp-consistency.png" alt="Unsupervised Template Warp Consistency for Implicit Surface Correspondences thumbnail">
              </div>
              
              <div class="pub-card__body">
                <h3 class="pub-card__title">Unsupervised Template Warp Consistency for Implicit Surface Correspondences</h3>
                <p class="pub-card__authors">Mengya Liu, <strong>Ajad Chhatkuli</strong>, Janis Postels, L. Gool, F. Tombari</p>
                <p class="pub-card__meta">
                  <em>Computer graphics forum (Print)</em>, 2023
                  
                </p>
                
                <div class="pub-card__actions">
                  <div class="pub-card__links">
                    
                    
                    
                    <a class="pub-link pub-link--text" href="https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/cgf.14745">pdf</a>
                    
                    
                    <details class="pub-card__details">
                      <summary class="pub-card__toggle">
                        <span>More</span>
                        <span class="pub-card__toggle-icon" aria-hidden="true">⏬</span>
                      </summary>
                      <div class="pub-card__excerpt">
                        <p>Unsupervised template discovery via implicit representation in a category of shapes has recently
shown strong performance.</p>


                      </div>
                    </details>
                    
                  </div>
                  
                </div>
                
              </div>
            </article>
            
            
            
            <article class="pub-card">
              
              <div class="pub-card__media">
                <img src="/tn/images/multivt-multiple-task-framework.png" alt="MultiVT: Multiple-Task Framework for Dentistry thumbnail">
              </div>
              
              <div class="pub-card__body">
                <h3 class="pub-card__title">MultiVT: Multiple-Task Framework for Dentistry</h3>
                <p class="pub-card__authors">Edoardo Mello Rella, <strong>Ajad Chhatkuli</strong>, E. Konukoglu, L. V. Gool</p>
                <p class="pub-card__meta">
                  <em>DART@MICCAI</em>, 2023
                  
                </p>
                
                <div class="pub-card__actions">
                  <div class="pub-card__links">
                    
                    
                    
                    
                    <details class="pub-card__details">
                      <summary class="pub-card__toggle">
                        <span>More</span>
                        <span class="pub-card__toggle-icon" aria-hidden="true">⏬</span>
                      </summary>
                      <div class="pub-card__excerpt">
                        <p>MultiVT introduces a unified transformer-based pipeline that handles detection, segmentation, and
landmark localisation on dental radiographs in a single network.</p>


                      </div>
                    </details>
                    
                  </div>
                  
                </div>
                
              </div>
            </article>
            
            
            
            <article class="pub-card">
              
              <div class="pub-card__media">
                <img src="/tn/images/tada.png" alt="TADA: Taxonomy Adaptive Domain Adaptation thumbnail">
              </div>
              
              <div class="pub-card__body">
                <h3 class="pub-card__title">TADA: Taxonomy Adaptive Domain Adaptation</h3>
                <p class="pub-card__authors">Rui Gong, Martin Danelljan, Dengxin Dai, Wenguan Wang, Danda Pani Paudel, <strong>Ajad Chhatkuli</strong>, Fisher Yu, Luc Van Gool</p>
                <p class="pub-card__meta">
                  <em>To appear in ECCV</em>, 2022
                  
                </p>
                
                <div class="pub-card__actions">
                  <div class="pub-card__links">
                    
                    
                    
                    <a class="pub-link pub-link--text" href="https://arxiv.org/pdf/2109.04813">arxiv</a>
                    
                    
                    <details class="pub-card__details">
                      <summary class="pub-card__toggle">
                        <span>More</span>
                        <span class="pub-card__toggle-icon" aria-hidden="true">⏬</span>
                      </summary>
                      <div class="pub-card__excerpt">
                        <p>We propose a few shot domain adaptive method for incorporating source and target data which have inconsistent label spaces as well as different input domains. On the label-level, we employ a bilateral mixed sampling strategy to augment the target domain, and a relabelling method to unify and align the label spaces. We address the image-level domain gap by proposing an uncertainty-rectified contrastive learning method, leading to more domain-invariant and class discriminative features.</p>

                      </div>
                    </details>
                    
                  </div>
                  
                </div>
                
              </div>
            </article>
            
            
            
            <article class="pub-card">
              
              <div class="pub-card__media">
                <img src="/tn/images/neural_vecfield.png" alt="Neural Vector Fields for Surface Representation and Inference thumbnail">
              </div>
              
              <div class="pub-card__body">
                <h3 class="pub-card__title">Neural Vector Fields for Surface Representation and Inference</h3>
                <p class="pub-card__authors">Edoardo Mello Rella, <strong>Ajad Chhatkuli</strong>, Ender Konukoglu, Luc Van Gool</p>
                <p class="pub-card__meta">
                  <em>CoRR</em>, 2022
                  
                </p>
                
                <div class="pub-card__actions">
                  <div class="pub-card__links">
                    
                    
                    
                    <a class="pub-link pub-link--text" href="https://arxiv.org/pdf/2204.06552.pdf">arxiv</a>
                    
                    
                    <details class="pub-card__details">
                      <summary class="pub-card__toggle">
                        <span>More</span>
                        <span class="pub-card__toggle-icon" aria-hidden="true">⏬</span>
                      </summary>
                      <div class="pub-card__excerpt">
                        <p>We propose a novel representation for implicit surfaces. This is the first method that can represent open surfaces and also obtain mesh representation with a feedforward network and a Marching Cubes like algorithm.</p>

                      </div>
                    </details>
                    
                  </div>
                  
                </div>
                
              </div>
            </article>
            
            
            
            <article class="pub-card">
              
              <div class="pub-card__media">
                <img src="/tn/images/zerovec.png" alt="Zero pixel directional boundary by vector transform thumbnail">
              </div>
              
              <div class="pub-card__body">
                <h3 class="pub-card__title">Zero pixel directional boundary by vector transform</h3>
                <p class="pub-card__authors">Edoardo Mello Rella, <strong>Ajad Chhatkuli</strong>, Yun Liu, Ender Konukoglu, Luc Van Gool</p>
                <p class="pub-card__meta">
                  <em>ICLR</em>, 2022
                  
                </p>
                
                <div class="pub-card__actions">
                  <div class="pub-card__links">
                    
                    
                    
                    <a class="pub-link pub-link--text" href="https://openreview.net/pdf?id=nxcABL7jbQh">pdf</a>
                    
                    
                    <details class="pub-card__details">
                      <summary class="pub-card__toggle">
                        <span>More</span>
                        <span class="pub-card__toggle-icon" aria-hidden="true">⏬</span>
                      </summary>
                      <div class="pub-card__excerpt">
                        <p>We revisit the boundary detection problem and introduce a principled approach based on a redefinition of boundary with dense labels without label imbalance. We specifically use the unit vector to the closest boundary, beating standard binary label based methods and a baseline method based on distance transform in thorough experiments.</p>

                      </div>
                    </details>
                    
                  </div>
                  
                </div>
                
              </div>
            </article>
            
            
            
            <article class="pub-card">
              
              <div class="pub-card__media">
                <img src="/tn/images/zippypoint.png" alt="ZippyPoint: Fast Interest Point Detection, Description, and Matching through Mixed Precision Discretization thumbnail">
              </div>
              
              <div class="pub-card__body">
                <h3 class="pub-card__title">ZippyPoint: Fast Interest Point Detection, Description, and Matching through Mixed Precision Discretization</h3>
                <p class="pub-card__authors">*Simon Mauer, Menelaos Kanakis*, Matteo Spallanzani, <strong>Ajad Chhatkuli</strong>, Luc Van Gool</p>
                <p class="pub-card__meta">
                  <em>CoRR</em>, 2022
                  
                </p>
                
                <div class="pub-card__actions">
                  <div class="pub-card__links">
                    
                    
                    
                    <a class="pub-link pub-link--text" href="https://arxiv.org/pdf/2203.03610">arxiv</a>
                    
                    
                    <details class="pub-card__details">
                      <summary class="pub-card__toggle">
                        <span>More</span>
                        <span class="pub-card__toggle-icon" aria-hidden="true">⏬</span>
                      </summary>
                      <div class="pub-card__excerpt">
                        <p>We propose a method for training a fast binary local image point descriptor. The paper describes a method to train binary descriptor by adapting network quantization techniques. The method achieves unprecedented speed in deep local image descriptors.</p>

                      </div>
                    </details>
                    
                  </div>
                  
                </div>
                
              </div>
            </article>
            
            
            
            <article class="pub-card">
              
              <div class="pub-card__media">
                <img src="/tn/images/unsup-nonrigid.png" alt="Unsupervised Monocular Depth Reconstruction of Non-Rigid Scenes thumbnail">
              </div>
              
              <div class="pub-card__body">
                <h3 class="pub-card__title">Unsupervised Monocular Depth Reconstruction of Non-Rigid Scenes</h3>
                <p class="pub-card__authors">Ayça Takmaz, Danda Pani Paudel, Thomas Probst, <strong>Ajad Chhatkuli</strong>, Martin R Oswald, Luc Van Gool</p>
                <p class="pub-card__meta">
                  <em>3DV</em>, 2021
                  
                </p>
                
                <div class="pub-card__actions">
                  <div class="pub-card__links">
                    
                    
                    
                    <a class="pub-link pub-link--text" href="https://arxiv.org/pdf/2012.15680">arxiv</a>
                    
                    
                    <details class="pub-card__details">
                      <summary class="pub-card__toggle">
                        <span>More</span>
                        <span class="pub-card__toggle-icon" aria-hidden="true">⏬</span>
                      </summary>
                      <div class="pub-card__excerpt">
                        <p>We present an unsupervised monocular framework for dense depth estimation of
dynamic scenes, which jointly reconstructs rigid and nonrigid parts without explicitly modelling the camera motion. Our method uses the as rigid as possible deformation prior.</p>

                      </div>
                    </details>
                    
                  </div>
                  
                </div>
                
              </div>
            </article>
            
            
            
            <article class="pub-card">
              
              <div class="pub-card__media">
                <img src="/tn/images/mocda.png" alt="Cluster, split, fuse, and update: Meta-learning for open compound domain adaptive semantic segmentation thumbnail">
              </div>
              
              <div class="pub-card__body">
                <h3 class="pub-card__title">Cluster, split, fuse, and update: Meta-learning for open compound domain adaptive semantic segmentation</h3>
                <p class="pub-card__authors">Rui Gong, Yuhua Chen, Danda Pani Paudel, Yawei Li,  <strong>Ajad Chhatkuli</strong>, Wen Li, Dengxin Dai, Luc Van Gool</p>
                <p class="pub-card__meta">
                  <em>CVPR</em>, 2021
                  
                </p>
                
                <div class="pub-card__actions">
                  <div class="pub-card__links">
                    
                    
                    
                    <a class="pub-link pub-link--text" href="https://openaccess.thecvf.com/content/CVPR2021/papers/Gong_Cluster_Split_Fuse_and_Update_Meta-Learning_for_Open_Compound_Domain_CVPR_2021_paper.pdf">pdf</a>
                    
                    
                    <details class="pub-card__details">
                      <summary class="pub-card__toggle">
                        <span>More</span>
                        <span class="pub-card__toggle-icon" aria-hidden="true">⏬</span>
                      </summary>
                      <div class="pub-card__excerpt">
                        <p>The paper tackles the challenging problem of Open Compound Domain Adaptation (OCDA), where target domain is modeled as
a compound of multiple unknown homogeneous domains. We propose a principled meta-learning based approach to OCDA for semantic segmentation, MOCDA, by modeling the unlabeled target domain continuously.</p>

                      </div>
                    </details>
                    
                  </div>
                  
                </div>
                
              </div>
            </article>
            
            
            
            <article class="pub-card">
              
              <div class="pub-card__media">
                <img src="/tn/images/cgan_knowledge.png" alt="Efficient conditional gan transfer with knowledge propagation across classes thumbnail">
              </div>
              
              <div class="pub-card__body">
                <h3 class="pub-card__title">Efficient conditional gan transfer with knowledge propagation across classes</h3>
                <p class="pub-card__authors">Mohamad Shahbazi, Zhiwu Huang, Danda Pani Paudel, <strong>Ajad Chhatkuli</strong>, Luc Van Gool</p>
                <p class="pub-card__meta">
                  <em>CVPR</em>, 2021
                  
                </p>
                
                <div class="pub-card__actions">
                  <div class="pub-card__links">
                    
                    
                    
                    <a class="pub-link pub-link--text" href="http://openaccess.thecvf.com/content/CVPR2021/papers/Shahbazi_Efficient_Conditional_GAN_Transfer_With_Knowledge_Propagation_Across_Classes_CVPR_2021_paper.pdf">pdf</a>
                    
                    
                    <details class="pub-card__details">
                      <summary class="pub-card__toggle">
                        <span>More</span>
                        <span class="pub-card__toggle-icon" aria-hidden="true">⏬</span>
                      </summary>
                      <div class="pub-card__excerpt">
                        <p>We introduce a new GAN transfer method to explicitly propagate
the knowledge from the old classes to the new classes. The
key idea is to enforce the popularly used conditional batch
normalization (BN) to learn the class-specific information
of the new classes from that of the old classes, with implicit
knowledge sharing among the new ones. This allows for
an efficient knowledge propagation from the old classes to
the new ones, with the BN parameters increasing linearly
with the number of new classes.</p>

                      </div>
                    </details>
                    
                  </div>
                  
                </div>
                
              </div>
            </article>
            
            
            
            <article class="pub-card">
              
              <div class="pub-card__media">
                <img src="/tn/images/mhsa.png" alt="Transformer in convolutional neural networks thumbnail">
              </div>
              
              <div class="pub-card__body">
                <h3 class="pub-card__title">Transformer in convolutional neural networks</h3>
                <p class="pub-card__authors">Yun Liu, Yun Liu, Guolei Sun, Yu Qiu, Le Zhang, <strong>Ajad Chhatkuli</strong>, Luc Van Gool</p>
                <p class="pub-card__meta">
                  <em>CVPR</em>, 2021
                  
                </p>
                
                <div class="pub-card__actions">
                  <div class="pub-card__links">
                    
                    
                    
                    <a class="pub-link pub-link--text" href="https://arxiv.org/pdf/2106.03180">arxiv</a>
                    
                    
                    <details class="pub-card__details">
                      <summary class="pub-card__toggle">
                        <span>More</span>
                        <span class="pub-card__toggle-icon" aria-hidden="true">⏬</span>
                      </summary>
                      <div class="pub-card__excerpt">
                        <p>This paper tackles the low-efficiency flaw of the vision transformer caused by the high computational/space complexity in
Multi-Head Self-Attention (MHSA). To this end, we propose the Hierarchical MHSA (H-MHSA), whose representation is computed in a
hierarchical manner. Specifically, we first divide the input image into patches as commonly done, and each patch is viewed as a token. Then, the proposed H-MHSA learns token relationships within local patches, serving as local relationship modeling. Then, the small
patches are merged into larger ones, and H-MHSA models the global dependencies for the small number of the merged tokens. At last,
the local and global attentive features are aggregated to obtain features with powerful representation capacity.</p>

                      </div>
                    </details>
                    
                  </div>
                  
                </div>
                
              </div>
            </article>
            
            
            
            <article class="pub-card">
              
              <div class="pub-card__media">
                <img src="/tn/images/mio.png" alt="Learning Condition Invariant Features for Retrieval-Based Localization from 1M Images thumbnail">
              </div>
              
              <div class="pub-card__body">
                <h3 class="pub-card__title">Learning Condition Invariant Features for Retrieval-Based Localization from 1M Images</h3>
                <p class="pub-card__authors">Janine Thoma, Danda Pani Paudel, <strong>Ajad Chhatkuli</strong>, Luc Van Gool</p>
                <p class="pub-card__meta">
                  <em>CoRR</em>, 2020
                  
                </p>
                
                <div class="pub-card__actions">
                  <div class="pub-card__links">
                    
                    
                    
                    <a class="pub-link pub-link--text" href="https://arxiv.org/pdf/2008.12165">arxiv</a>
                    
                    
                    <details class="pub-card__details">
                      <summary class="pub-card__toggle">
                        <span>More</span>
                        <span class="pub-card__toggle-icon" aria-hidden="true">⏬</span>
                      </summary>
                      <div class="pub-card__excerpt">
                        <p>In this paper, we train and evaluate several localization methods on three different benchmark datasets, including Oxford RobotCar with over one million images.
This large scale evaluation yields valuable insights into
the generalizability and performance of retrieval-based
localization. Based on our findings, we develop a novel
method for learning more accurate and better generalizing localization features. It consists of two main
contributions: (i) a feature volume-based loss function,
and (ii) hard positive and pairwise negative mining.</p>

                      </div>
                    </details>
                    
                  </div>
                  
                </div>
                
              </div>
            </article>
            
            
            
            <article class="pub-card">
              
              <div class="pub-card__media">
                <img src="/tn/images/cat_keyp.png" alt="Unsupervised learning of category-specific symmetric 3d keypoints from point sets thumbnail">
              </div>
              
              <div class="pub-card__body">
                <h3 class="pub-card__title">Unsupervised learning of category-specific symmetric 3d keypoints from point sets</h3>
                <p class="pub-card__authors">Clara Fernandez-Labrador, <strong>Ajad Chhatkuli</strong>, Danda Pani Paudel, Jose J Guerrero, Cédric Demonceaux, Luc Van Gool</p>
                <p class="pub-card__meta">
                  <em>ECCV</em>, 2020
                  
                </p>
                
                <div class="pub-card__actions">
                  <div class="pub-card__links">
                    
                    
                    
                    <a class="pub-link pub-link--text" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123700545.pdf">pdf</a>
                    
                    
                    <details class="pub-card__details">
                      <summary class="pub-card__toggle">
                        <span>More</span>
                        <span class="pub-card__toggle-icon" aria-hidden="true">⏬</span>
                      </summary>
                      <div class="pub-card__excerpt">
                        <p>This paper aims at learning semantic 3D keypoints across misaligned shapes in a category, in an unsupervised manner. In order to do so, we
model shapes defined by the keypoints, within a category, using the symmetric linear basis shapes without assuming the plane of symmetry to be
known. The plane of symmetry and the basis shapes are learned as weights of the network for a category, while the coefficients are predicted per shape instance.</p>

                      </div>
                    </details>
                    
                  </div>
                  
                  <div class="pub-card__links pub-card__links--secondary">
                    
                    
                    <a class="pub-link pub-link--text" href="https://github.com/cfernandezlab/Category-Specific-Keypoints">code</a>
                    
                    
                    
                  </div>
                  
                </div>
                
              </div>
            </article>
            
            
            
            <article class="pub-card">
              
              <div class="pub-card__media">
                <img src="/tn/images/sc_rsfm.png" alt="Self-Calibration Supported Robust Projective Structure-from-Motion thumbnail">
              </div>
              
              <div class="pub-card__body">
                <h3 class="pub-card__title">Self-Calibration Supported Robust Projective Structure-from-Motion</h3>
                <p class="pub-card__authors">Rui Gong, Danda Pani Paudel, <strong>Ajad Chhatkuli</strong>, Luc Van Gool</p>
                <p class="pub-card__meta">
                  <em>CoRR</em>, 2020
                  
                </p>
                
                <div class="pub-card__actions">
                  <div class="pub-card__links">
                    
                    
                    
                    <a class="pub-link pub-link--text" href="https://arxiv.org/pdf/2007.02045">arxiv</a>
                    
                    
                    <details class="pub-card__details">
                      <summary class="pub-card__toggle">
                        <span>More</span>
                        <span class="pub-card__toggle-icon" aria-hidden="true">⏬</span>
                      </summary>
                      <div class="pub-card__excerpt">
                        <p>In this paper, we propose a unified SfM method, in which the matching process is supported by self-calibration constraints. We use the idea that good matches should yield a valid calibration. In this process, we make use of the Dual Image of Absolute Quadric projection equations within a multiview correspondence framework, in order to obtain robust matching from a set of putative correspondences.</p>

                      </div>
                    </details>
                    
                  </div>
                  
                </div>
                
              </div>
            </article>
            
            
            
            <article class="pub-card">
              
              <div class="pub-card__media">
                <img src="/tn/images/geo_mappable.png" alt="Geometrically mappable image features thumbnail">
              </div>
              
              <div class="pub-card__body">
                <h3 class="pub-card__title">Geometrically mappable image features</h3>
                <p class="pub-card__authors">Janine Thoma, Danda Pani Paudel, <strong>Ajad Chhatkuli</strong>, Luc Van Gool</p>
                <p class="pub-card__meta">
                  <em>RAL</em>, 2020
                  
                </p>
                
                <div class="pub-card__actions">
                  <div class="pub-card__links">
                    
                    
                    
                    <a class="pub-link pub-link--text" href="https://ieeexplore.ieee.org/iel7/7083369/7339444/08977317.pdf">pdf</a>
                    
                    
                    <details class="pub-card__details">
                      <summary class="pub-card__toggle">
                        <span>More</span>
                        <span class="pub-card__toggle-icon" aria-hidden="true">⏬</span>
                      </summary>
                      <div class="pub-card__excerpt">
                        <p>In this work, we propose a method that learns image features targeted for image-retrieval-based localization. Retrieval-based localization has several benefits, such as easy maintenance and quick computation. However, the state-of-the-art features only provide visual similarity scores which do not explicitly reveal the geometric distance between query and retrieved images. Knowing this distance is highly desirable for accurate localization, especially when the reference images are sparsely distributed in the scene. Therefore, we propose a novel loss function for learning image features which are both visually representative and geometrically relatable.</p>

                      </div>
                    </details>
                    
                  </div>
                  
                </div>
                
              </div>
            </article>
            
            
            
            <article class="pub-card">
              
              <div class="pub-card__media">
                <img src="/tn/images/conv_relax.png" alt="Convex relaxations for consensus and non-minimal problems in 3D vision thumbnail">
              </div>
              
              <div class="pub-card__body">
                <h3 class="pub-card__title">Convex relaxations for consensus and non-minimal problems in 3D vision</h3>
                <p class="pub-card__authors">Thomas Probst, Danda Pani Paudel, <strong>Ajad Chhatkuli</strong>, Luc Van Gool</p>
                <p class="pub-card__meta">
                  <em>ICCV</em>, 2019
                  
                </p>
                
                <div class="pub-card__actions">
                  <div class="pub-card__links">
                    
                    
                    
                    <a class="pub-link pub-link--text" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Probst_Convex_Relaxations_for_Consensus_and_Non-Minimal_Problems_in_3D_Vision_ICCV_2019_paper.pdf">pdf</a>
                    
                    
                    <details class="pub-card__details">
                      <summary class="pub-card__toggle">
                        <span>More</span>
                        <span class="pub-card__toggle-icon" aria-hidden="true">⏬</span>
                      </summary>
                      <div class="pub-card__excerpt">
                        <p>The proposed method exploits the well known Shor’s or
Lasserre’s relaxations, whose theoretical aspects are also
discussed. Notably, we further exploit the Polynomials Optimization Problems (POP) formulation
of non-minimal solver also for the generic consensus maximization problems in 3D vision. We support the proposed framework
by three diverse applications in 3D vision, namely rigid
body transformation estimation, Non-Rigid Structure-fromMotion (NRSfM), and camera autocalibration</p>

                      </div>
                    </details>
                    
                  </div>
                  
                </div>
                
              </div>
            </article>
            
            
            
            <article class="pub-card">
              
              <div class="pub-card__media">
                <img src="/tn/images/unsup_consensus.png" alt="Unsupervised learning of consensus maximization for 3d vision problems thumbnail">
              </div>
              
              <div class="pub-card__body">
                <h3 class="pub-card__title">Unsupervised learning of consensus maximization for 3d vision problems</h3>
                <p class="pub-card__authors">Thomas Probst, Danda Pani Paudel, <strong>Ajad Chhatkuli</strong>, Luc Van Gool</p>
                <p class="pub-card__meta">
                  <em>CVPR</em>, 2019
                  
                </p>
                
                <div class="pub-card__actions">
                  <div class="pub-card__links">
                    
                    
                    
                    <a class="pub-link pub-link--text" href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Probst_Unsupervised_Learning_of_Consensus_Maximization_for_3D_Vision_Problems_CVPR_2019_paper.pdf">pdf</a>
                    
                    
                    <details class="pub-card__details">
                      <summary class="pub-card__toggle">
                        <span>More</span>
                        <span class="pub-card__toggle-icon" aria-hidden="true">⏬</span>
                      </summary>
                      <div class="pub-card__excerpt">
                        <p>In this paper, we propose for the
first time an unsupervised learning framework for consensus maximization, in the context of solving 3D vision problems. For that purpose, we establish a relationship between inlier measurements, represented by an ideal of inlier set, and the subspace of polynomials representing the
space of target transformations. Using this relationship, we
derive a constraint that must be satisfied by the sought inlier set.</p>

                      </div>
                    </details>
                    
                  </div>
                  
                </div>
                
              </div>
            </article>
            
            
            
            <article class="pub-card">
              
              <div class="pub-card__media">
                <img src="/tn/images/what_corresp.png" alt="What Correspondences Reveal About Unknown Camera and Motion Models? thumbnail">
              </div>
              
              <div class="pub-card__body">
                <h3 class="pub-card__title">What Correspondences Reveal About Unknown Camera and Motion Models?</h3>
                <p class="pub-card__authors">Thomas Probst, <strong>Ajad Chhatkuli</strong>, Danda Pani Paudel, Luc Van Gool</p>
                <p class="pub-card__meta">
                  <em>CVPR</em>, 2019
                  
                </p>
                
                <div class="pub-card__actions">
                  <div class="pub-card__links">
                    
                    
                    
                    <a class="pub-link pub-link--text" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Probst_What_Correspondences_Reveal_About_Unknown_Camera_and_Motion_Models_CVPR_2019_paper.pdf">pdf</a>
                    
                    
                    <details class="pub-card__details">
                      <summary class="pub-card__toggle">
                        <span>More</span>
                        <span class="pub-card__toggle-icon" aria-hidden="true">⏬</span>
                      </summary>
                      <div class="pub-card__excerpt">
                        <p>The work describes finding a particular camera geometry from two view image correspondences. We first describe a framework that can be used to compute the `simplest’ camera model for a given set of correspondences. We further provide a theoretical analysis on what type of motions and camera models are discoverable from two view correspondences.</p>

                      </div>
                    </details>
                    
                  </div>
                  
                </div>
                
              </div>
            </article>
            
            
            
            <article class="pub-card">
              
              <div class="pub-card__media">
                <img src="/tn/images/map_nav_plan.png" alt="Mapping, localization and path planning for image-based navigation using visual features and map thumbnail">
              </div>
              
              <div class="pub-card__body">
                <h3 class="pub-card__title">Mapping, localization and path planning for image-based navigation using visual features and map</h3>
                <p class="pub-card__authors">Janine Thoma, Danda Pani Paudel, <strong>Ajad Chhatkuli</strong>, Thomas Probst, Luc Van Gool</p>
                <p class="pub-card__meta">
                  <em>CVPR</em>, 2019
                  
                </p>
                
                <div class="pub-card__actions">
                  <div class="pub-card__links">
                    
                    
                    
                    <a class="pub-link pub-link--text" href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Thoma_Mapping_Localization_and_Path_Planning_for_Image-Based_Navigation_Using_Visual_CVPR_2019_paper.pdf">pdf</a>
                    
                    
                    <details class="pub-card__details">
                      <summary class="pub-card__toggle">
                        <span>More</span>
                        <span class="pub-card__toggle-icon" aria-hidden="true">⏬</span>
                      </summary>
                      <div class="pub-card__excerpt">
                        <p>The problem of localization often arises as part of a navigation process. In this paper we summarize
the reference images as a set of landmarks, which meet the
requirements for image-based navigation. A contribution
of this paper is to formulate such a set of requirements for
the two sub-tasks involved: compact map construction and
accurate self localization. These requirements are then exploited for compact map representation and accurate self-localization, using the framework of a network flow problem. During this process, we formulate the map construction and self-localization problems as convex quadratic and second-order cone programs, respectively.</p>

                      </div>
                    </details>
                    
                  </div>
                  
                </div>
                
              </div>
            </article>
            
            
            
            <article class="pub-card">
              
              <div class="pub-card__media">
                <img src="/tn/images/model-free.png" alt="Model-free consensus maximization for non-rigid shapes thumbnail">
              </div>
              
              <div class="pub-card__body">
                <h3 class="pub-card__title">Model-free consensus maximization for non-rigid shapes</h3>
                <p class="pub-card__authors">Thomas Probst, <strong>Ajad Chhatkuli</strong>, Danda Pani Paudel, Luc Van Gool</p>
                <p class="pub-card__meta">
                  <em>ECCV</em>, 2018
                  
                </p>
                
                <div class="pub-card__actions">
                  <div class="pub-card__links">
                    
                    
                    
                    <a class="pub-link pub-link--text" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Thomas_Probst_Model-free_Consensus_Maximization_ECCV_2018_paper.pdf">pdf</a>
                    
                    
                    <details class="pub-card__details">
                      <summary class="pub-card__toggle">
                        <span>More</span>
                        <span class="pub-card__toggle-icon" aria-hidden="true">⏬</span>
                      </summary>
                      <div class="pub-card__excerpt">
                        <p>We formulate the model-free consensus
maximization as an Integer Program in a graph using ‘rules’ on measurements.
We then provide a method to solve it optimally using the Branch and Bound
(BnB) paradigm. We focus its application on non-rigid shapes, where we apply
the method to remove outlier 3D correspondences and achieve performance superior to the state of the art.</p>

                      </div>
                    </details>
                    
                  </div>
                  
                </div>
                
              </div>
            </article>
            
            
            
            <article class="pub-card">
              
              <div class="pub-card__media">
                <img src="/tn/images/inc-nrsfm.png" alt="Incremental non-rigid structure-from-motion with unknown focal length thumbnail">
              </div>
              
              <div class="pub-card__body">
                <h3 class="pub-card__title">Incremental non-rigid structure-from-motion with unknown focal length</h3>
                <p class="pub-card__authors">Thomas Probst, Danda Pani Paudel, <strong>Ajad Chhatkuli</strong>, Luc Van Gool</p>
                <p class="pub-card__meta">
                  <em>ECCV</em>, 2018
                  
                </p>
                
                <div class="pub-card__actions">
                  <div class="pub-card__links">
                    
                    
                    
                    <a class="pub-link pub-link--text" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Thomas_Probst_Incremental_Non-Rigid_Structure-from-Motion_ECCV_2018_paper.pdf">pdf</a>
                    
                    
                    <details class="pub-card__details">
                      <summary class="pub-card__toggle">
                        <span>More</span>
                        <span class="pub-card__toggle-icon" aria-hidden="true">⏬</span>
                      </summary>
                      <div class="pub-card__excerpt">
                        <p>In this paper we present
a method for incremental Non-Rigid Structure-from-Motion (NRSfM) with the
perspective camera model and the isometric surface prior with unknown focal
length. In the template-based case, we provide a method to estimate four parameters of the camera intrinsics. For the template-less scenario of NRSfM, we propose a method to upgrade reconstructions obtained for one focal length to another
based on local rigidity and the so-called Maximum Depth Heuristics (MDH). On
its basis we propose a method to simultaneously recover the focal length and the
non-rigid shapes. We further solve the problem of incorporating a large number of
points and adding more views in MDH-based NRSfM and efficiently solve them
with Second-Order Cone Programming (SOCP).</p>

                      </div>
                    </details>
                    
                  </div>
                  
                </div>
                
              </div>
            </article>
            
            
            
            <article class="pub-card">
              
              <div class="pub-card__media">
                <img src="/tn/images/robot-eye.png" alt="Automatic tool landmark detection for stereo vision in robot-assisted retinal surgery thumbnail">
              </div>
              
              <div class="pub-card__body">
                <h3 class="pub-card__title">Automatic tool landmark detection for stereo vision in robot-assisted retinal surgery</h3>
                <p class="pub-card__authors">Thomas Probst, Danda Pani Paudel, <strong>Ajad Chhatkuli</strong>, Luc Van Gool</p>
                <p class="pub-card__meta">
                  <em>ICRA/RAL</em>, 2018
                  
                </p>
                
                <div class="pub-card__actions">
                  <div class="pub-card__links">
                    
                    
                    
                    <a class="pub-link pub-link--text" href="https://arxiv.org/abs/1709.05665">arxiv</a>
                    
                    
                    <details class="pub-card__details">
                      <summary class="pub-card__toggle">
                        <span>More</span>
                        <span class="pub-card__toggle-icon" aria-hidden="true">⏬</span>
                      </summary>
                      <div class="pub-card__excerpt">
                        <p>In this paper, we solve the problem of the calibration of stereo-microscope and consequently that of the 3D reconstruction of an unknown scene under the microscope. For the first time using a single pipeline, starting from uncalibrated cameras we achieve the metric 3D reconstruction and registration, for retinal microsurgery. The key ingredients of our method are: (a) surgical tool landmark detection, and (b) 3D reconstruction with the stereo microscope, using the detected landmarks. To address the former, we propose a novel deep learning method that detects and recognizes keypoints in high definition images at higher than real-time speed. We use the detected 2D keypoints along with their corresponding 3D coordinates obtained from the robot sensors to calibrate the stereo microscope using an affine projection model. We design an online 3D reconstruction pipeline that makes use of smoothness constraints and performs robot-to-camera registration.</p>

                      </div>
                    </details>
                    
                  </div>
                  
                </div>
                
              </div>
            </article>
            
            
            
            <article class="pub-card">
              
              <div class="pub-card__media">
                <img src="/tn/images/tlmdh.png" alt="Inextensible non-rigid structure-from-motion by second-order cone programming thumbnail">
              </div>
              
              <div class="pub-card__body">
                <h3 class="pub-card__title">Inextensible non-rigid structure-from-motion by second-order cone programming</h3>
                <p class="pub-card__authors"><strong>Ajad Chhatkuli</strong>, Daniel Pizarro, Toby Collins, Adrien Bartoli</p>
                <p class="pub-card__meta">
                  <em>T-PAMI</em>, 2017
                  
                </p>
                
                <div class="pub-card__actions">
                  <div class="pub-card__links">
                    
                    
                    
                    <a class="pub-link pub-link--text" href="https://ieeexplore.ieee.org/iel7/34/8454009/08067444.pdf">pdf</a>
                    
                    
                    <details class="pub-card__details">
                      <summary class="pub-card__toggle">
                        <span>More</span>
                        <span class="pub-card__toggle-icon" aria-hidden="true">⏬</span>
                      </summary>
                      <div class="pub-card__excerpt">
                        <p>We present a global and convex formulation for the template-less 3D reconstruction of a deforming object with the perspective camera. We show for the first time how to construct a Second-Order Cone Programming (SOCP) problem for Non-Rigid Structure-from-Motion (NRSfM) using the Maximum-Depth Heuristic (MDH). In this regard, we deviate strongly from the general trend of using affine cameras and factorization-based methods to solve NRSfM, which do not perform well with complex nonlinear deformations. In MDH, the points’ depths are maximized so that the distance between neighbouring points in camera space are upper bounded by the geodesic distance. In NRSfM both geodesic and camera space distances are unknown. We show that, nonetheless, given point correspondences and the camera’s intrinsics the whole problem can be solved with SOCP. This is the first convex formulation for NRSfM with physical constraints. We further present how robustness and temporal continuity can be included in the formulation to handle outliers and decrease the problem size, respectively.</p>

                      </div>
                    </details>
                    
                  </div>
                  
                </div>
                
              </div>
            </article>
            
            
            
            <article class="pub-card">
              
              <div class="pub-card__media">
                <img src="/tn/images/mdh-nrsfm.png" alt="Inextensible non-rigid shape-from-motion by second-order cone programming thumbnail">
              </div>
              
              <div class="pub-card__body">
                <h3 class="pub-card__title">Inextensible non-rigid shape-from-motion by second-order cone programming</h3>
                <p class="pub-card__authors"><strong>Ajad Chhatkuli</strong>, Daniel Pizarro, Toby Collins, Adrien Bartoli</p>
                <p class="pub-card__meta">
                  <em>CVPR</em>, 2016
                  
                </p>
                
                <div class="pub-card__actions">
                  <div class="pub-card__links">
                    
                    
                    
                    <a class="pub-link pub-link--text" href="https://openaccess.thecvf.com/content_cvpr_2016/papers/Chhatkuli_Inextensible_Non-Rigid_Shape-From-Motion_CVPR_2016_paper.pdf">pdf</a>
                    
                    
                    <details class="pub-card__details">
                      <summary class="pub-card__toggle">
                        <span>More</span>
                        <span class="pub-card__toggle-icon" aria-hidden="true">⏬</span>
                      </summary>
                      <div class="pub-card__excerpt">
                        <p>We present a global and convex formulation for
template-less 3D reconstruction of a deforming object with
the perspective camera. We show for the first time how
to construct a Second-Order Cone Programming (SOCP)
problem for Non-Rigid Shape-from-Motion (NRSfM) using
the Maximum-Depth Heuristic (MDH). In this regard, we
deviate strongly from the general trend of using affine cameras and factorization-based methods to solve NRSfM. In
MDH, the points’ depths are maximized so that the distance
between neighbouring points in camera space are upper
bounded by the geodesic distance. In NRSfM both geodesic
and camera space distances are unknown. We show that,
nonetheless, given point correspondences and the camera’s
intrinsics the whole problem is convex and solvable with
SOCP.</p>

                      </div>
                    </details>
                    
                  </div>
                  
                </div>
                
              </div>
            </article>
            
            
            
            <article class="pub-card">
              
              <div class="pub-card__media">
                <img src="/tn/images/stable-analytic.png" alt="A stable analytical framework for isometric shape-from-template by surface integration thumbnail">
              </div>
              
              <div class="pub-card__body">
                <h3 class="pub-card__title">A stable analytical framework for isometric shape-from-template by surface integration</h3>
                <p class="pub-card__authors"><strong>Ajad Chhatkuli</strong>, Daniel Pizarro, Adrien Bartoli, Toby Collins</p>
                <p class="pub-card__meta">
                  <em>T-PAMI</em>, 2016
                  
                </p>
                
                <div class="pub-card__actions">
                  <div class="pub-card__links">
                    
                    
                    
                    <a class="pub-link pub-link--text" href="https://ieeexplore.ieee.org/iel7/34/4359286/07464873.pdf">pdf</a>
                    
                    
                    <details class="pub-card__details">
                      <summary class="pub-card__toggle">
                        <span>More</span>
                        <span class="pub-card__toggle-icon" aria-hidden="true">⏬</span>
                      </summary>
                      <div class="pub-card__excerpt">
                        <p>Shape-from-Template (SfT) reconstructs the shape of a deforming surface from a single image, a 3D template and a deformation prior. For isometric deformations, this is a well-posed problem. However, previous methods which require no initialization break down when the perspective effects are small, which happens when the object is small or viewed from larger distances. That is, they do not handle all projection geometries. We propose stable SfT methods that accurately reconstruct the 3D shape for all projection geometries. We follow the existing approach of using first-order differential constraints and obtain local analytical solutions for depth and the first-order quantities: the depth-gradient or the surface normal. Previous methods use the depth solution directly to obtain the 3D shape. We prove that the depth solution is unstable when the projection geometry tends to affine, while the solution for the first-order quantities remain stable for all projection geometries. We therefore propose to solve SfT by first estimating the first-order quantities (either depth-gradient or surface normal) and integrating them to obtain shape.</p>

                      </div>
                    </details>
                    
                  </div>
                  
                </div>
                
              </div>
            </article>
            
            
            
            <article class="pub-card">
              
              <div class="pub-card__media">
                <img src="/tn/images/inftpl_nrsfm.png" alt="Non-Rigid Shape-from-Motion for Isometric Surfaces using Infinitesimal Planarity thumbnail">
              </div>
              
              <div class="pub-card__body">
                <h3 class="pub-card__title">Non-Rigid Shape-from-Motion for Isometric Surfaces using Infinitesimal Planarity</h3>
                <p class="pub-card__authors"><strong>Ajad Chhatkuli</strong>, Daniel Pizarro, Adrien Bartoli</p>
                <p class="pub-card__meta">
                  <em>BMVC</em>, 2014
                  
                </p>
                
                <div class="pub-card__actions">
                  <div class="pub-card__links">
                    
                    
                    
                    <a class="pub-link pub-link--text" href="http://www.bmva.org/bmvc/2014/files/paper041.pdf">pdf</a>
                    
                    
                    <details class="pub-card__details">
                      <summary class="pub-card__toggle">
                        <span>More</span>
                        <span class="pub-card__toggle-icon" aria-hidden="true">⏬</span>
                      </summary>
                      <div class="pub-card__excerpt">
                        <p>This paper proposes a general framework to solve Non-Rigid Shape-from-Motion
(NRSfM) with the perspective camera under isometric deformations. Contrary to the
usual low-rank linear shape basis, isometry allows us to recover complex shape deformations from a sparse set of images. Existing methods suffer from ambiguities and may be
very expensive to solve. We bring four main contributions. First, we formulate isometric NRSfM as a system of first-order Partial Differential Equations (PDE) involving the
shape’s depth and normal field and an unknown template. Second, we show this system
cannot be locally resolved. Third, we introduce the concept of infinitesimal planarity and
show that it makes the system locally solvable for at least three views. Fourth, we derive
an analytic solution which involves convex, linear least-squares optimization only, and
outperforms existing works.</p>

                      </div>
                    </details>
                    
                  </div>
                  
                </div>
                
              </div>
            </article>
            
            
            
            <article class="pub-card">
              
              <div class="pub-card__media">
                <img src="/tn/images/laparo-uterine.png" alt="Live image parsing in uterine laparoscopy thumbnail">
              </div>
              
              <div class="pub-card__body">
                <h3 class="pub-card__title">Live image parsing in uterine laparoscopy</h3>
                <p class="pub-card__authors"><strong>Ajad Chhatkuli</strong>, Adrien Bartoli, Abed Malti, Toby Collins</p>
                <p class="pub-card__meta">
                  <em>ISBI</em>, 2014
                  
                </p>
                
                <div class="pub-card__actions">
                  <div class="pub-card__links">
                    
                    
                    
                    <a class="pub-link pub-link--text" href="https://ieeexplore.ieee.org/iel7/6861559/6867780/06868106.pdf">pdf</a>
                    
                    
                    <details class="pub-card__details">
                      <summary class="pub-card__toggle">
                        <span>More</span>
                        <span class="pub-card__toggle-icon" aria-hidden="true">⏬</span>
                      </summary>
                      <div class="pub-card__excerpt">
                        <p>Augmented Reality (AR) can improve the information delivery to surgeons. In laparosurgery, the primary goal of AR is to provide multimodal information overlaid in live laparoscopic videos. For gynecologic laparoscopy, the 3D reconstruction of uterus and its deformable registration to preoperative data form the major problems in AR. Shape-from-Shading (SfS) and inter-frame registration require an accurate identification of the uterus region, the occlusions due to surgical tools, specularities, and other tissues. We propose a cascaded patient-specific real-time segmentation method to identify these four important regions. We use a color based Gaussian Mixture Model (GMM) to segment the tools and a more elaborate color and texture model to segment the uterus. The specularities are obtained by a saturation test. We show that our segmentation improves SfS and inter-frame registration of the uterus.</p>

                      </div>
                    </details>
                    
                  </div>
                  
                </div>
                
              </div>
            </article>
            
            
            
            <article class="pub-card">
              
              <div class="pub-card__media">
                <img src="/tn/images/iso-stablesft.png" alt="Stable template-based isometric 3D reconstruction in all imaging conditions by linear least-squares thumbnail">
              </div>
              
              <div class="pub-card__body">
                <h3 class="pub-card__title">Stable template-based isometric 3D reconstruction in all imaging conditions by linear least-squares</h3>
                <p class="pub-card__authors"><strong>Ajad Chhatkuli</strong>, Daniel Pizarro, Adrien Bartoli</p>
                <p class="pub-card__meta">
                  <em>CVPR</em>, 2014
                  
                </p>
                
                <div class="pub-card__actions">
                  <div class="pub-card__links">
                    
                    
                    
                    <a class="pub-link pub-link--text" href="https://openaccess.thecvf.com/content_cvpr_2014/papers/Chhatkuli_Stable_Template-Based_Isometric_2014_CVPR_paper.pdf">pdf</a>
                    
                    
                    <details class="pub-card__details">
                      <summary class="pub-card__toggle">
                        <span>More</span>
                        <span class="pub-card__toggle-icon" aria-hidden="true">⏬</span>
                      </summary>
                      <div class="pub-card__excerpt">
                        <p>Reconstructing an isometric surface from a single 2D input image matched to a 3D
template has been shown to be a well-posed problem. This however does not
tell us how reconstruction algorithms will behave in practical conditions, where the amount of perspective is generally
small and the projection thus behaves like weak-perspective
or orthography. We here bring answers to what is theoretically recoverable in such imaging conditions, and explain
why existing convex numerical solutions and analytical solutions to 3D reconstruction may be unstable. We then propose a new algorithm which works under all imaging conditions, from strong to loose perspective by using the algebraic solution of the depth’s Jacobian.</p>

                      </div>
                    </details>
                    
                  </div>
                  
                </div>
                
              </div>
            </article>
            
            
            
            <article class="pub-card">
              
              <div class="pub-card__media">
                <img src="/tn/images/com-figs.png" alt="Separating compound figures in journal articles to allow for subfigure classification thumbnail">
              </div>
              
              <div class="pub-card__body">
                <h3 class="pub-card__title">Separating compound figures in journal articles to allow for subfigure classification</h3>
                <p class="pub-card__authors"><strong>Ajad Chhatkuli</strong>, Antonio Foncubierta-Rodríguez, Dimitrios Markonis, Fabrice Meriaudeau, Henning Müller</p>
                <p class="pub-card__meta">
                  <em>SPIE</em>, 2013
                  
                </p>
                
                <div class="pub-card__actions">
                  <div class="pub-card__links">
                    
                    
                    
                    <a class="pub-link pub-link--text" href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/8674/86740J/Separating-compound-figures-in-journal-articles-to-allow-for-subfigure/10.1117/12.2007897.pdf">pdf</a>
                    
                    
                    <details class="pub-card__details">
                      <summary class="pub-card__toggle">
                        <span>More</span>
                        <span class="pub-card__toggle-icon" aria-hidden="true">⏬</span>
                      </summary>
                      <div class="pub-card__excerpt">
                        <p>Journal images represent an important part of the knowledge stored in the medical literature. Figure classification has received much attention as the information of the image types can be used in a variety of contexts to focus image search and filter out unwanted information or ”noise”, for example non–clinical images. A major problem in figure classification is the fact that many figures in the biomedical literature are compound figures and do often contain more than a single figure type. Some journals do separate compound figures into several parts but many do not, thus requiring currently manual separation. In this work, a technique of compound figure separation is proposed and implemented based on systematic detection and analysis of uniform space gaps. The method discussed in this article is evaluated on a dataset of journal figures of the open access literature that was created for the ImageCLEF 2012 benchmark and contains about 3000 compound figures. Automatic tools can easily reach a relatively high accuracy in separating compound figures. To further increase accuracy efforts are needed to improve the detection process as well as to avoid over–separation with powerful analysis strategies.</p>

                      </div>
                    </details>
                    
                  </div>
                  
                </div>
                
              </div>
            </article>
            
            
          </div>
        </section>
      </main>

      <footer class="site-footer">
        <p>Design and source code adapted from <a href="https://jonbarron.info">Jon Barron's website</a> and <a href="https://leonidk.com/">Leonid Keselman's</a> <a href="https://github.com/leonidk/new_website">Jekyll fork</a>.</p>
      </footer>
    </div>
  </div>
</body>

</html>
