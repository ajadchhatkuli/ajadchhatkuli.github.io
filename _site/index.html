<!DOCTYPE HTML>
<html lang="en">

<head>
  <title>Ajad Chhatkuli</title>

  <meta content="text/html; charset=utf-8" http-equiv="Content-Type">

  <meta name="author" content="Ajad Chhatkuli" />
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="/style.css" />
  <link rel="canonical" href="http://localhost:4000/">
  <link href="https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css">

</head>



<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <h1>
                Ajad Chhatkuli
              </h1>
              <p>I am a postdoc at CVL <a href="https://vision.ee.ethz.ch/">Computer Vision Lab</a> in ETH Zurich, group of <a href="https://ee.ethz.ch/the-department/faculty/professors/person-detail.OTAyMzM=.TGlzdC80MTEsMTA1ODA0MjU5.html">Prof. Luc Van Gool</a>. Before that I completed my PhD <a href="https://tel.archives-ouvertes.fr/tel-01491344">thesis</a> under the supervision of <a href="http://igt.ip.uca.fr/~ab/">Prof. Adrien Bartoli</a> and <a href="https://www.uah.es/en/estudios/profesor/Daniel-Pizarro-Perez/">Prof. Daniel Pizarro</a> in the field of non-rigid 3D reconstruction. Currently my research scope is divided in two aspects: i) learning mid to high level understanding from images and  ii) 3D shape analysis and 3D implicit functions.
              </p>
              
              <p style="text-align:center">
                <a href="mailto:ajad.chhatkuli@vision.ee.ethz.ch"> Email</a> &nbsp;/&nbsp;
                <a href="https://github.com/a-cvision">GitHub</a> &nbsp;/&nbsp;
                <a href="https://scholar.google.com/citations?user=3BHMHU4AAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img style="width:100%;max-width:100%" alt="profile photo" src="images/mypic.JPG">
            </td>
          </tr>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:100%;vertical-align:middle">
              <h2>Publications</h2>
            </td>
          </tr>
        </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">

          
          
          
          <tr>
            <td style="padding:2.5%;width:50%;vertical-align:middle;min-width:180px">
              <img src="/tn/images/tada.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>TADA: Taxonomy Adaptive Domain Adaptation</h3>
              <br>
              Rui Gong, Martin Danelljan, Dengxin Dai, Wenguan Wang, Danda Pani Paudel, <strong>Ajad Chhatkuli</strong>, Fisher Yu, Luc Van Gool

              <br>
              <em>To appear in ECCV</em>, 2022 
              <br>
              
              <a href="https://arxiv.org/pdf/2109.04813">arxiv</a> /
              
              
              
              
              
              
              <p></p>
              <p>We propose a few shot domain adaptive method for incorporating source and target data which have inconsistent label spaces as well as different input domains. On the label-level, we employ a bilateral mixed sampling strategy to augment the target domain, and a relabelling method to unify and align the label spaces. We address the image-level domain gap by proposing an uncertainty-rectified contrastive learning method, leading to more domain-invariant and class discriminative features.</p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:50%;vertical-align:middle;min-width:180px">
              <img src="/tn/images/neural_vecfield.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Neural Vector Fields for Surface Representation and Inference</h3>
              <br>
              Edoardo Mello Rella, <strong>Ajad Chhatkuli</strong>, Ender Konukoglu, Luc Van Gool

              <br>
              <em>CoRR</em>, 2022 
              <br>
              
              <a href="https://arxiv.org/pdf/2204.06552.pdf">arxiv</a> /
              
              
              
              
              
              
              <p></p>
              <p>We propose a novel representation for implicit surfaces. This is the first method that can represent open surfaces and also obtain mesh representation with a feedforward network and a Marching Cubes like algorithm.</p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:50%;vertical-align:middle;min-width:180px">
              <img src="/tn/images/zerovec.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Zero pixel directional boundary by vector transform</h3>
              <br>
              Edoardo Mello Rella, <strong>Ajad Chhatkuli</strong>, Yun Liu, Ender Konukoglu, Luc Van Gool

              <br>
              <em>ICLR</em>, 2022 
              <br>
              
              
              <a href="https://openreview.net/pdf?id=nxcABL7jbQh">pdf</a> /
              
              
              
              
              
              <p></p>
              <p>We revisit the boundary detection problem and introduce a principled approach based on a redefinition of boundary with dense labels without label imbalance. We specifically use the unit vector to the closest boundary, beating standard binary label based methods and a baseline method based on distance transform in thorough experiments.</p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:50%;vertical-align:middle;min-width:180px">
              <img src="/tn/images/zippypoint.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>ZippyPoint: Fast Interest Point Detection, Description, and Matching through Mixed Precision Discretization</h3>
              <br>
              *Simon Mauer, Menelaos Kanakis*, Matteo Spallanzani, <strong>Ajad Chhatkuli</strong>, Luc Van Gool

              <br>
              <em>CoRR</em>, 2022 
              <br>
              
              <a href="https://arxiv.org/pdf/2203.03610">arxiv</a> /
              
              
              
              
              
              
              <p></p>
              <p>We propose a method for training a fast binary local image point descriptor. The paper describes a method to train binary descriptor by adapting network quantization techniques. The method achieves unprecedented speed in deep local image descriptors.</p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:50%;vertical-align:middle;min-width:180px">
              <img src="/tn/images/unsup-nonrigid.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Unsupervised Monocular Depth Reconstruction of Non-Rigid Scenes</h3>
              <br>
              Ayça Takmaz, Danda Pani Paudel, Thomas Probst, <strong>Ajad Chhatkuli</strong>, Martin R Oswald, Luc Van Gool

              <br>
              <em>3DV</em>, 2021 
              <br>
              
              <a href="https://arxiv.org/pdf/2012.15680">arxiv</a> /
              
              
              
              
              
              
              <p></p>
              <p>We present an unsupervised monocular framework for dense depth estimation of
dynamic scenes, which jointly reconstructs rigid and nonrigid parts without explicitly modelling the camera motion. Our method uses the as rigid as possible deformation prior.</p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:50%;vertical-align:middle;min-width:180px">
              <img src="/tn/images/mocda.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Cluster, split, fuse, and update: Meta-learning for open compound domain adaptive semantic segmentation</h3>
              <br>
              Rui Gong, Yuhua Chen, Danda Pani Paudel, Yawei Li,  <strong>Ajad Chhatkuli</strong>, Wen Li, Dengxin Dai, Luc Van Gool

              <br>
              <em>CVPR</em>, 2021 
              <br>
              
              
              <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Gong_Cluster_Split_Fuse_and_Update_Meta-Learning_for_Open_Compound_Domain_CVPR_2021_paper.pdf">pdf</a> /
              
              
              
              
              
              <p></p>
              <p>The paper tackles the challenging problem of Open Compound Domain Adaptation (OCDA), where target domain is modeled as
a compound of multiple unknown homogeneous domains. We propose a principled meta-learning based approach to OCDA for semantic segmentation, MOCDA, by modeling the unlabeled target domain continuously.</p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:50%;vertical-align:middle;min-width:180px">
              <img src="/tn/images/cgan_knowledge.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Efficient conditional gan transfer with knowledge propagation across classes</h3>
              <br>
              Mohamad Shahbazi, Zhiwu Huang, Danda Pani Paudel, <strong>Ajad Chhatkuli</strong>, Luc Van Gool

              <br>
              <em>CVPR</em>, 2021 
              <br>
              
              
              <a href="http://openaccess.thecvf.com/content/CVPR2021/papers/Shahbazi_Efficient_Conditional_GAN_Transfer_With_Knowledge_Propagation_Across_Classes_CVPR_2021_paper.pdf">pdf</a> /
              
              
              
              
              
              <p></p>
              <p>We introduce a new GAN transfer method to explicitly propagate
the knowledge from the old classes to the new classes. The
key idea is to enforce the popularly used conditional batch
normalization (BN) to learn the class-specific information
of the new classes from that of the old classes, with implicit
knowledge sharing among the new ones. This allows for
an efficient knowledge propagation from the old classes to
the new ones, with the BN parameters increasing linearly
with the number of new classes.</p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:50%;vertical-align:middle;min-width:180px">
              <img src="/tn/images/mhsa.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Transformer in convolutional neural networks</h3>
              <br>
              Yun Liu, Yun Liu, Guolei Sun, Yu Qiu, Le Zhang, <strong>Ajad Chhatkuli</strong>, Luc Van Gool

              <br>
              <em>CVPR</em>, 2021 
              <br>
              
              <a href="https://arxiv.org/pdf/2106.03180">arxiv</a> /
              
              
              
              
              
              
              <p></p>
              <p>This paper tackles the low-efficiency flaw of the vision transformer caused by the high computational/space complexity in
Multi-Head Self-Attention (MHSA). To this end, we propose the Hierarchical MHSA (H-MHSA), whose representation is computed in a
hierarchical manner. Specifically, we first divide the input image into patches as commonly done, and each patch is viewed as a token. Then, the proposed H-MHSA learns token relationships within local patches, serving as local relationship modeling. Then, the small
patches are merged into larger ones, and H-MHSA models the global dependencies for the small number of the merged tokens. At last,
the local and global attentive features are aggregated to obtain features with powerful representation capacity.</p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:50%;vertical-align:middle;min-width:180px">
              <img src="/tn/images/mio.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Learning Condition Invariant Features for Retrieval-Based Localization from 1M Images</h3>
              <br>
              Janine Thoma, Danda Pani Paudel, <strong>Ajad Chhatkuli</strong>, Luc Van Gool

              <br>
              <em>CoRR</em>, 2020 
              <br>
              
              <a href="https://arxiv.org/pdf/2008.12165">arxiv</a> /
              
              
              
              
              
              
              <p></p>
              <p>In this paper, we train and evaluate several localization methods on three different benchmark datasets, including Oxford RobotCar with over one million images.
This large scale evaluation yields valuable insights into
the generalizability and performance of retrieval-based
localization. Based on our findings, we develop a novel
method for learning more accurate and better generalizing localization features. It consists of two main
contributions: (i) a feature volume-based loss function,
and (ii) hard positive and pairwise negative mining.</p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:50%;vertical-align:middle;min-width:180px">
              <img src="/tn/images/cat_keyp.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Unsupervised learning of category-specific symmetric 3d keypoints from point sets</h3>
              <br>
              Clara Fernandez-Labrador, <strong>Ajad Chhatkuli</strong>, Danda Pani Paudel, Jose J Guerrero, Cédric Demonceaux, Luc Van Gool

              <br>
              <em>ECCV</em>, 2020 
              <br>
              
              
              <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123700545.pdf">pdf</a> /
              
              
              
              <a href="https://github.com/cfernandezlab/Category-Specific-Keypoints">code</a> /
              
              
              
              <p></p>
              <p>This paper aims at learning semantic 3D keypoints across misaligned shapes in a category, in an unsupervised manner. In order to do so, we
model shapes defined by the keypoints, within a category, using the symmetric linear basis shapes without assuming the plane of symmetry to be
known. The plane of symmetry and the basis shapes are learned as weights of the network for a category, while the coefficients are predicted per shape instance.</p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:50%;vertical-align:middle;min-width:180px">
              <img src="/tn/images/sc_rsfm.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Self-Calibration Supported Robust Projective Structure-from-Motion</h3>
              <br>
              Rui Gong, Danda Pani Paudel, <strong>Ajad Chhatkuli</strong>, Luc Van Gool

              <br>
              <em>CoRR</em>, 2020 
              <br>
              
              <a href="https://arxiv.org/pdf/2007.02045">arxiv</a> /
              
              
              
              
              
              
              <p></p>
              <p>In this paper, we propose a unified SfM method, in which the matching process is supported by self-calibration constraints. We use the idea that good matches should yield a valid calibration. In this process, we make use of the Dual Image of Absolute Quadric projection equations within a multiview correspondence framework, in order to obtain robust matching from a set of putative correspondences.</p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:50%;vertical-align:middle;min-width:180px">
              <img src="/tn/images/geo_mappable.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Geometrically mappable image features</h3>
              <br>
              Janine Thoma, Danda Pani Paudel, <strong>Ajad Chhatkuli</strong>, Luc Van Gool

              <br>
              <em>RAL</em>, 2020 
              <br>
              
              
              <a href="https://ieeexplore.ieee.org/iel7/7083369/7339444/08977317.pdf">pdf</a> /
              
              
              
              
              
              <p></p>
              <p>In this work, we propose a method that learns image features targeted for image-retrieval-based localization. Retrieval-based localization has several benefits, such as easy maintenance and quick computation. However, the state-of-the-art features only provide visual similarity scores which do not explicitly reveal the geometric distance between query and retrieved images. Knowing this distance is highly desirable for accurate localization, especially when the reference images are sparsely distributed in the scene. Therefore, we propose a novel loss function for learning image features which are both visually representative and geometrically relatable.</p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:50%;vertical-align:middle;min-width:180px">
              <img src="/tn/images/conv_relax.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Convex relaxations for consensus and non-minimal problems in 3D vision</h3>
              <br>
              Thomas Probst, Danda Pani Paudel, <strong>Ajad Chhatkuli</strong>, Luc Van Gool

              <br>
              <em>ICCV</em>, 2019 
              <br>
              
              
              <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Probst_Convex_Relaxations_for_Consensus_and_Non-Minimal_Problems_in_3D_Vision_ICCV_2019_paper.pdf">pdf</a> /
              
              
              
              
              
              <p></p>
              <p>The proposed method exploits the well known Shor’s or
Lasserre’s relaxations, whose theoretical aspects are also
discussed. Notably, we further exploit the Polynomials Optimization Problems (POP) formulation
of non-minimal solver also for the generic consensus maximization problems in 3D vision. We support the proposed framework
by three diverse applications in 3D vision, namely rigid
body transformation estimation, Non-Rigid Structure-fromMotion (NRSfM), and camera autocalibration</p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:50%;vertical-align:middle;min-width:180px">
              <img src="/tn/images/unsup_consensus.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Unsupervised learning of consensus maximization for 3d vision problems</h3>
              <br>
              Thomas Probst, Danda Pani Paudel, <strong>Ajad Chhatkuli</strong>, Luc Van Gool

              <br>
              <em>CVPR</em>, 2019 
              <br>
              
              
              <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Probst_Unsupervised_Learning_of_Consensus_Maximization_for_3D_Vision_Problems_CVPR_2019_paper.pdf">pdf</a> /
              
              
              
              
              
              <p></p>
              <p>In this paper, we propose for the
first time an unsupervised learning framework for consensus maximization, in the context of solving 3D vision problems. For that purpose, we establish a relationship between inlier measurements, represented by an ideal of inlier set, and the subspace of polynomials representing the
space of target transformations. Using this relationship, we
derive a constraint that must be satisfied by the sought inlier set.</p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:50%;vertical-align:middle;min-width:180px">
              <img src="/tn/images/what_corresp.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>What Correspondences Reveal About Unknown Camera and Motion Models?</h3>
              <br>
              Thomas Probst, <strong>Ajad Chhatkuli</strong>, Danda Pani Paudel, Luc Van Gool

              <br>
              <em>CVPR</em>, 2019 
              <br>
              
              
              <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Probst_What_Correspondences_Reveal_About_Unknown_Camera_and_Motion_Models_CVPR_2019_paper.pdf">pdf</a> /
              
              
              
              
              
              <p></p>
              <p>The work describes finding a particular camera geometry from two view image correspondences. We first describe a framework that can be used to compute the `simplest’ camera model for a given set of correspondences. We further provide a theoretical analysis on what type of motions and camera models are discoverable from two view correspondences.</p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:50%;vertical-align:middle;min-width:180px">
              <img src="/tn/images/map_nav_plan.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Mapping, localization and path planning for image-based navigation using visual features and map</h3>
              <br>
              Janine Thoma, Danda Pani Paudel, <strong>Ajad Chhatkuli</strong>, Thomas Probst, Luc Van Gool

              <br>
              <em>CVPR</em>, 2019 
              <br>
              
              
              <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Thoma_Mapping_Localization_and_Path_Planning_for_Image-Based_Navigation_Using_Visual_CVPR_2019_paper.pdf">pdf</a> /
              
              
              
              
              
              <p></p>
              <p>The problem of localization often arises as part of a navigation process. In this paper we summarize
the reference images as a set of landmarks, which meet the
requirements for image-based navigation. A contribution
of this paper is to formulate such a set of requirements for
the two sub-tasks involved: compact map construction and
accurate self localization. These requirements are then exploited for compact map representation and accurate self-localization, using the framework of a network flow problem. During this process, we formulate the map construction and self-localization problems as convex quadratic and second-order cone programs, respectively.</p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:50%;vertical-align:middle;min-width:180px">
              <img src="/tn/images/model-free.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Model-free consensus maximization for non-rigid shapes</h3>
              <br>
              Thomas Probst, <strong>Ajad Chhatkuli</strong>, Danda Pani Paudel, Luc Van Gool

              <br>
              <em>ECCV</em>, 2018 
              <br>
              
              
              <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Thomas_Probst_Model-free_Consensus_Maximization_ECCV_2018_paper.pdf">pdf</a> /
              
              
              
              
              
              <p></p>
              <p>We formulate the model-free consensus
maximization as an Integer Program in a graph using ‘rules’ on measurements.
We then provide a method to solve it optimally using the Branch and Bound
(BnB) paradigm. We focus its application on non-rigid shapes, where we apply
the method to remove outlier 3D correspondences and achieve performance superior to the state of the art.</p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:50%;vertical-align:middle;min-width:180px">
              <img src="/tn/images/inc-nrsfm.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Incremental non-rigid structure-from-motion with unknown focal length</h3>
              <br>
              Thomas Probst, Danda Pani Paudel, <strong>Ajad Chhatkuli</strong>, Luc Van Gool

              <br>
              <em>ECCV</em>, 2018 
              <br>
              
              
              <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Thomas_Probst_Incremental_Non-Rigid_Structure-from-Motion_ECCV_2018_paper.pdf">pdf</a> /
              
              
              
              
              
              <p></p>
              <p>In this paper we present
a method for incremental Non-Rigid Structure-from-Motion (NRSfM) with the
perspective camera model and the isometric surface prior with unknown focal
length. In the template-based case, we provide a method to estimate four parameters of the camera intrinsics. For the template-less scenario of NRSfM, we propose a method to upgrade reconstructions obtained for one focal length to another
based on local rigidity and the so-called Maximum Depth Heuristics (MDH). On
its basis we propose a method to simultaneously recover the focal length and the
non-rigid shapes. We further solve the problem of incorporating a large number of
points and adding more views in MDH-based NRSfM and efficiently solve them
with Second-Order Cone Programming (SOCP).</p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:50%;vertical-align:middle;min-width:180px">
              <img src="/tn/images/robot-eye.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Automatic tool landmark detection for stereo vision in robot-assisted retinal surgery</h3>
              <br>
              Thomas Probst, Danda Pani Paudel, <strong>Ajad Chhatkuli</strong>, Luc Van Gool

              <br>
              <em>ICRA/RAL</em>, 2018 
              <br>
              
              <a href="https://arxiv.org/abs/1709.05665">arxiv</a> /
              
              
              
              
              
              
              <p></p>
              <p>In this paper, we solve the problem of the calibration of stereo-microscope and consequently that of the 3D reconstruction of an unknown scene under the microscope. For the first time using a single pipeline, starting from uncalibrated cameras we achieve the metric 3D reconstruction and registration, for retinal microsurgery. The key ingredients of our method are: (a) surgical tool landmark detection, and (b) 3D reconstruction with the stereo microscope, using the detected landmarks. To address the former, we propose a novel deep learning method that detects and recognizes keypoints in high definition images at higher than real-time speed. We use the detected 2D keypoints along with their corresponding 3D coordinates obtained from the robot sensors to calibrate the stereo microscope using an affine projection model. We design an online 3D reconstruction pipeline that makes use of smoothness constraints and performs robot-to-camera registration.</p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:50%;vertical-align:middle;min-width:180px">
              <img src="/tn/images/tlmdh.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Inextensible non-rigid structure-from-motion by second-order cone programming</h3>
              <br>
              <strong>Ajad Chhatkuli</strong>, Daniel Pizarro, Toby Collins, Adrien Bartoli

              <br>
              <em>T-PAMI</em>, 2017 
              <br>
              
              
              <a href="https://ieeexplore.ieee.org/iel7/34/8454009/08067444.pdf">pdf</a> /
              
              
              
              
              
              <p></p>
              <p>We present a global and convex formulation for the template-less 3D reconstruction of a deforming object with the perspective camera. We show for the first time how to construct a Second-Order Cone Programming (SOCP) problem for Non-Rigid Structure-from-Motion (NRSfM) using the Maximum-Depth Heuristic (MDH). In this regard, we deviate strongly from the general trend of using affine cameras and factorization-based methods to solve NRSfM, which do not perform well with complex nonlinear deformations. In MDH, the points’ depths are maximized so that the distance between neighbouring points in camera space are upper bounded by the geodesic distance. In NRSfM both geodesic and camera space distances are unknown. We show that, nonetheless, given point correspondences and the camera’s intrinsics the whole problem can be solved with SOCP. This is the first convex formulation for NRSfM with physical constraints. We further present how robustness and temporal continuity can be included in the formulation to handle outliers and decrease the problem size, respectively.</p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:50%;vertical-align:middle;min-width:180px">
              <img src="/tn/images/mdh-nrsfm.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Inextensible non-rigid shape-from-motion by second-order cone programming</h3>
              <br>
              <strong>Ajad Chhatkuli</strong>, Daniel Pizarro, Toby Collins, Adrien Bartoli

              <br>
              <em>CVPR</em>, 2016 
              <br>
              
              
              <a href="https://openaccess.thecvf.com/content_cvpr_2016/papers/Chhatkuli_Inextensible_Non-Rigid_Shape-From-Motion_CVPR_2016_paper.pdf">pdf</a> /
              
              
              
              
              
              <p></p>
              <p>We present a global and convex formulation for
template-less 3D reconstruction of a deforming object with
the perspective camera. We show for the first time how
to construct a Second-Order Cone Programming (SOCP)
problem for Non-Rigid Shape-from-Motion (NRSfM) using
the Maximum-Depth Heuristic (MDH). In this regard, we
deviate strongly from the general trend of using affine cameras and factorization-based methods to solve NRSfM. In
MDH, the points’ depths are maximized so that the distance
between neighbouring points in camera space are upper
bounded by the geodesic distance. In NRSfM both geodesic
and camera space distances are unknown. We show that,
nonetheless, given point correspondences and the camera’s
intrinsics the whole problem is convex and solvable with
SOCP.</p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:50%;vertical-align:middle;min-width:180px">
              <img src="/tn/images/stable-analytic.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>A stable analytical framework for isometric shape-from-template by surface integration</h3>
              <br>
              <strong>Ajad Chhatkuli</strong>, Daniel Pizarro, Adrien Bartoli, Toby Collins

              <br>
              <em>T-PAMI</em>, 2016 
              <br>
              
              
              <a href="https://ieeexplore.ieee.org/iel7/34/4359286/07464873.pdf">pdf</a> /
              
              
              
              
              
              <p></p>
              <p>Shape-from-Template (SfT) reconstructs the shape of a deforming surface from a single image, a 3D template and a deformation prior. For isometric deformations, this is a well-posed problem. However, previous methods which require no initialization break down when the perspective effects are small, which happens when the object is small or viewed from larger distances. That is, they do not handle all projection geometries. We propose stable SfT methods that accurately reconstruct the 3D shape for all projection geometries. We follow the existing approach of using first-order differential constraints and obtain local analytical solutions for depth and the first-order quantities: the depth-gradient or the surface normal. Previous methods use the depth solution directly to obtain the 3D shape. We prove that the depth solution is unstable when the projection geometry tends to affine, while the solution for the first-order quantities remain stable for all projection geometries. We therefore propose to solve SfT by first estimating the first-order quantities (either depth-gradient or surface normal) and integrating them to obtain shape.</p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:50%;vertical-align:middle;min-width:180px">
              <img src="/tn/images/inftpl_nrsfm.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Non-Rigid Shape-from-Motion for Isometric Surfaces using Infinitesimal Planarity</h3>
              <br>
              <strong>Ajad Chhatkuli</strong>, Daniel Pizarro, Adrien Bartoli

              <br>
              <em>BMVC</em>, 2014 
              <br>
              
              
              <a href="http://www.bmva.org/bmvc/2014/files/paper041.pdf">pdf</a> /
              
              
              
              
              
              <p></p>
              <p>This paper proposes a general framework to solve Non-Rigid Shape-from-Motion
(NRSfM) with the perspective camera under isometric deformations. Contrary to the
usual low-rank linear shape basis, isometry allows us to recover complex shape deformations from a sparse set of images. Existing methods suffer from ambiguities and may be
very expensive to solve. We bring four main contributions. First, we formulate isometric NRSfM as a system of first-order Partial Differential Equations (PDE) involving the
shape’s depth and normal field and an unknown template. Second, we show this system
cannot be locally resolved. Third, we introduce the concept of infinitesimal planarity and
show that it makes the system locally solvable for at least three views. Fourth, we derive
an analytic solution which involves convex, linear least-squares optimization only, and
outperforms existing works.</p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:50%;vertical-align:middle;min-width:180px">
              <img src="/tn/images/laparo-uterine.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Live image parsing in uterine laparoscopy</h3>
              <br>
              <strong>Ajad Chhatkuli</strong>, Adrien Bartoli, Abed Malti, Toby Collins

              <br>
              <em>ISBI</em>, 2014 
              <br>
              
              
              <a href="https://ieeexplore.ieee.org/iel7/6861559/6867780/06868106.pdf">pdf</a> /
              
              
              
              
              
              <p></p>
              <p>Augmented Reality (AR) can improve the information delivery to surgeons. In laparosurgery, the primary goal of AR is to provide multimodal information overlaid in live laparoscopic videos. For gynecologic laparoscopy, the 3D reconstruction of uterus and its deformable registration to preoperative data form the major problems in AR. Shape-from-Shading (SfS) and inter-frame registration require an accurate identification of the uterus region, the occlusions due to surgical tools, specularities, and other tissues. We propose a cascaded patient-specific real-time segmentation method to identify these four important regions. We use a color based Gaussian Mixture Model (GMM) to segment the tools and a more elaborate color and texture model to segment the uterus. The specularities are obtained by a saturation test. We show that our segmentation improves SfS and inter-frame registration of the uterus.</p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:50%;vertical-align:middle;min-width:180px">
              <img src="/tn/images/iso-stablesft.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Stable template-based isometric 3D reconstruction in all imaging conditions by linear least-squares</h3>
              <br>
              <strong>Ajad Chhatkuli</strong>, Daniel Pizarro, Adrien Bartoli

              <br>
              <em>CVPR</em>, 2014 
              <br>
              
              
              <a href="https://openaccess.thecvf.com/content_cvpr_2014/papers/Chhatkuli_Stable_Template-Based_Isometric_2014_CVPR_paper.pdf">pdf</a> /
              
              
              
              
              
              <p></p>
              <p>Reconstructing an isometric surface from a single 2D input image matched to a 3D
template has been shown to be a well-posed problem. This however does not
tell us how reconstruction algorithms will behave in practical conditions, where the amount of perspective is generally
small and the projection thus behaves like weak-perspective
or orthography. We here bring answers to what is theoretically recoverable in such imaging conditions, and explain
why existing convex numerical solutions and analytical solutions to 3D reconstruction may be unstable. We then propose a new algorithm which works under all imaging conditions, from strong to loose perspective by using the algebraic solution of the depth’s Jacobian.</p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:50%;vertical-align:middle;min-width:180px">
              <img src="/tn/images/compoundfigures.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Separating compound figures in journal articles to allow for subfigure classification</h3>
              <br>
              <strong>Ajad Chhatkuli</strong>, Antonio Foncubierta-Rodríguez, Dimitrios Markonis, Fabrice Meriaudeau, Henning Müller

              <br>
              <em>SPIE</em>, 2013 
              <br>
              
              
              <a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/8674/86740J/Separating-compound-figures-in-journal-articles-to-allow-for-subfigure/10.1117/12.2007897.pdf">pdf</a> /
              
              
              
              
              
              <p></p>
              <p>Journal images represent an important part of the knowledge stored in the medical literature. Figure classification has received much attention as the information of the image types can be used in a variety of contexts to focus image search and filter out unwanted information or ”noise”, for example non–clinical images. A major problem in figure classification is the fact that many figures in the biomedical literature are compound figures and do often contain more than a single figure type. Some journals do separate compound figures into several parts but many do not, thus requiring currently manual separation. In this work, a technique of compound figure separation is proposed and implemented based on systematic detection and analysis of uniform space gaps. The method discussed in this article is evaluated on a dataset of journal figures of the open access literature that was created for the ImageCLEF 2012 benchmark and contains about 3000 compound figures. Automatic tools can easily reach a relatively high accuracy in separating compound figures. To further increase accuracy efforts are needed to improve the detection process as well as to avoid over–separation with powerful analysis strategies.</p>

            </td>
          </tr>
          
          
          
        </table>
        <br>
        <br>
        <br>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Design and source code from <a style="font-size:small;" href="https://jonbarron.info">Jon Barron's website</a> and <a style="font-size:small;" href="https://leonidk.com/"> Leonid Keselman's</a> <a style="font-size:small;" href="https://github.com/leonidk/new_website">Jeckyll fork</a>.
              </p>
            </td>
          </tr>
        </table>
      </td>
    </tr>
  </table>
</body>

</html>

