---
layout: post
title: "EgoNight: Towards Egocentric Vision Understanding at Night with a Challenging Benchmark"
date:   2025-10-07
image: /images/egonight-towards-egocentric-vision-understanding-at-night-with-a-challenging-ben.png
categories: research
authors: "Deheng Zhang, Yuqian Fu, Runyi Yang, Yang Miao, Tianwen Qian, Xu Zheng, Guolei Sun, <strong>Ajad Chhatkuli</strong>, Xuanjing Huang, Yugen Jiang, L. V. Gool, D. Paudel"
venue: "arXiv"
arxiv: https://arxiv.org/abs/2510.06218
pdf: https://arxiv.org/pdf/2510.06218.pdf
---

Most existing benchmarks for egocentric vision understanding focus primarily on daytime scenarios,
overlooking the low-light conditions that are inevitable in real-world applications.

To investigate this gap, we present EgoNight, the first comprehensive benchmark for nighttime
egocentric vision, with visual question answering (VQA) as the core task.

A key feature of EgoNight is the introduction of day-night aligned videos, which enhance night
annotation quality using the daytime data and reveal clear performance gaps between lighting
conditions.

To achieve this, we collect both synthetic videos rendered by Blender and real-world recordings,
ensuring that scenes and actions are visually and temporally aligned.

Leveraging these paired videos, we construct EgoNight-VQA, supported by a novel day-augmented night
auto-labeling engine and refinement through extensive human verification.

Each QA pair is double-checked by annotators for reliability.
